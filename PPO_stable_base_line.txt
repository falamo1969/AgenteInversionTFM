

def train_PPO(env_train, model_name, timesteps=50000):
    """PPO model"""

    start = time.time()
    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)
    model.learn(total_timesteps=timesteps)
    end = time.time()

    model.save(f"{config.TRAINED_MODEL_DIR}/{model_name}")
    print('Training time (PPO): ', (end - start) / 60, ' minutes')
    return model
model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)
def train_PPO_baseline3(env_train, model_name, timesteps=50000):
    """PPO model"""
    start = time.time()
    model = PPO(
            policy=MlpPolicy,
            env=env_train,
            tensorboard_log=f"{config.TRAINED_MODEL_DIR}/{model_name}",
            verbose=1,
            policy_kwargs=None,
            **PPO_PARAMS
        )
    model.learn(total_timesteps=timesteps)
    end = time.time()

    model.save(f"{config.TRAINED_MODEL_DIR}/{model_name}")
    print('Training time (PPO)
Rubén Pérez Ibáñez9:14 PM

#stable_baselines 3
from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy